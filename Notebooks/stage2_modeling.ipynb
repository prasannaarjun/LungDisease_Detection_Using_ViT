{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-18T05:23:01.969307Z",
     "start_time": "2025-04-18T05:22:58.331666Z"
    }
   },
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())   # Should return True\n",
    "print(torch.cuda.get_device_name(0))  # Should show RTX 4060"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:23:06.599433Z",
     "start_time": "2025-04-18T05:23:04.067997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Stage 2 - Paths\n",
    "stage2_train_dir = \"../Dataset/stage2/train\"\n",
    "stage2_val_dir = \"../Dataset/stage2/val\"\n",
    "stage2_test_dir = \"../Dataset/stage2/test\"\n",
    "\n",
    "# Transforms (same as stage 1)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# Datasets and loaders\n",
    "train_dataset2 = datasets.ImageFolder(stage2_train_dir, transform=transform)\n",
    "val_dataset2 = datasets.ImageFolder(stage2_val_dir, transform=transform)\n",
    "test_dataset2 = datasets.ImageFolder(stage2_test_dir, transform=transform)\n",
    "\n",
    "train_loader2 = DataLoader(train_dataset2, batch_size=32, shuffle=True)\n",
    "val_loader2 = DataLoader(val_dataset2, batch_size=32)\n",
    "test_loader2 = DataLoader(test_dataset2, batch_size=32)\n",
    "\n",
    "# Class names\n",
    "class_names2 = train_dataset2.classes\n",
    "print(\"Stage 2 Classes:\", class_names2)\n",
    "\n"
   ],
   "id": "be4cf3863c197b78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Classes: ['Bacterial Pneumonia', 'Viral Pneumonia']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:23:42.153163Z",
     "start_time": "2025-04-18T05:23:09.868318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import ViTForImageClassification\n",
    "# Define model\n",
    "model2 = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=2  # Binary classification\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model2.to(device)\n"
   ],
   "id": "ba1bb355e32d04ec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:36:46.898098Z",
     "start_time": "2025-04-18T05:24:23.171766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "# Optimizer and loss\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=2e-5)\n",
    "criterion2 = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Early stopping params\n",
    "best_val_loss2 = float('inf')\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, 16):\n",
    "    model2.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    loop = tqdm(train_loader2, desc=f\"Epoch {epoch}/15\")\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer2.zero_grad()\n",
    "        outputs = model2(images).logits\n",
    "        loss = criterion2(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader2)\n",
    "\n",
    "    # Validation\n",
    "    model2.eval()\n",
    "    val_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader2:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model2(images).logits\n",
    "            loss = criterion2(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader2)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"\\nEpoch {epoch} finished. Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Check for improvement\n",
    "    if avg_val_loss < best_val_loss2:\n",
    "        best_val_loss2 = avg_val_loss\n",
    "        torch.save(model2.state_dict(), \"../Model/best_model_stage2.pth\")\n",
    "        print(f\" Best model saved at epoch {epoch}.\")\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(f\" Early stopping triggered after {epoch} epochs.\")\n",
    "            break\n"
   ],
   "id": "68c68ce787282099",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 76/76 [01:51<00:00,  1.46s/it, loss=0.381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 finished. Avg Train Loss: 0.5821\n",
      "Validation Loss: 0.5492, Accuracy: 71.70%\n",
      " Best model saved at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 76/76 [01:24<00:00,  1.11s/it, loss=0.497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 finished. Avg Train Loss: 0.4956\n",
      "Validation Loss: 0.4839, Accuracy: 77.68%\n",
      " Best model saved at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 76/76 [01:45<00:00,  1.39s/it, loss=0.496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 finished. Avg Train Loss: 0.3874\n",
      "Validation Loss: 0.4528, Accuracy: 81.67%\n",
      " Best model saved at epoch 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 76/76 [01:47<00:00,  1.41s/it, loss=0.389] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 finished. Avg Train Loss: 0.2918\n",
      "Validation Loss: 0.4831, Accuracy: 80.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 76/76 [01:42<00:00,  1.34s/it, loss=0.179] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 finished. Avg Train Loss: 0.2403\n",
      "Validation Loss: 0.4751, Accuracy: 79.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 76/76 [01:40<00:00,  1.33s/it, loss=0.717] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 finished. Avg Train Loss: 0.1283\n",
      "Validation Loss: 0.4951, Accuracy: 81.30%\n",
      " Early stopping triggered after 6 epochs.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T17:50:50.453486Z",
     "start_time": "2025-04-18T17:50:50.262905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Remove all non-private global variables\n",
    "for obj in list(globals().keys()):\n",
    "    if not obj.startswith(\"_\") and obj not in [\"gc\", \"torch\"]:\n",
    "        del globals()[obj]\n",
    "\n",
    "# Run garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n"
   ],
   "id": "55c3ef4a856e6e3c",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m     10\u001B[39m gc.collect()\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# Clear GPU memory\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m.\u001B[49m\u001B[43mempty_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m torch.cuda.ipc_collect()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\cuda\\memory.py:192\u001B[39m, in \u001B[36mempty_cache\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    181\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001B[39;00m\n\u001B[32m    182\u001B[39m \u001B[33;03mallocator so that those can be used in other GPU application and visible in\u001B[39;00m\n\u001B[32m    183\u001B[39m \u001B[33;03m`nvidia-smi`.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    189\u001B[39m \u001B[33;03m    more details about GPU memory management.\u001B[39;00m\n\u001B[32m    190\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    191\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_initialized():\n\u001B[32m--> \u001B[39m\u001B[32m192\u001B[39m     \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_C\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_cuda_emptyCache\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:38:47.672540Z",
     "start_time": "2025-04-18T05:38:27.387980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load best model\n",
    "model2.load_state_dict(torch.load(\"../Model/best_model_stage2.pth\",weights_only=True))\n",
    "model2.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader2:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model2(images).logits\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\" Final Test Accuracy (Stage 2): {test_accuracy:.2f}%\")\n"
   ],
   "id": "b421430109993544",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Final Test Accuracy (Stage 2): 81.14%\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T17:47:29.961335Z",
     "start_time": "2025-04-18T17:47:29.836790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model2.eval()\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Disable gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader2:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model2(images)\n",
    "        _, preds = torch.max(outputs.logits, 1)\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "# Generate and plot confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(cmap='Blues', ax=ax, xticks_rotation=45)\n",
    "plt.title(\"Confusion Matrix - Stage 2\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ],
   "id": "24effd26160fe12d",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m     12\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m val_loader2:\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m         images, labels = \u001B[43mimages\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m, labels.to(device)\n\u001B[32m     14\u001B[39m         outputs = model2(images)\n\u001B[32m     15\u001B[39m         _, preds = torch.max(outputs.logits, \u001B[32m1\u001B[39m)\n",
      "\u001B[31mRuntimeError\u001B[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "HyperParameter Tuning",
   "id": "ccf0b223304b841"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T18:04:58.870069Z",
     "start_time": "2025-04-18T18:04:58.852960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTForImageClassification\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.amp import GradScaler, autocast\n",
    "import csv\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16,32,64])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 5, 15)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 128, 512, step=64)\n",
    "    patience = 3\n",
    "\n",
    "    print(f\"\\n Trial {trial.number} | LR: {learning_rate:.1e} | BS: {batch_size} | Epochs: {num_epochs} | Hidden: {hidden_size}\")\n",
    "\n",
    "    # Model setup\n",
    "    model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=2)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(model.classifier.in_features, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, 2)\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset2, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset2, batch_size=batch_size)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=f\"../logs/stage2_trial{trial.number}\")\n",
    "    csv_path = f\"../logs/stage2_trial{trial.number}.csv\"\n",
    "    with open(csv_path, mode='w', newline='') as f:\n",
    "        writer_csv = csv.writer(f)\n",
    "        writer_csv.writerow([\"epoch\", \"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_model_state = None\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                with autocast(device_type='cuda'):\n",
    "                    outputs = model(images)\n",
    "                _, preds = torch.max(outputs.logits, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds)\n",
    "        recall = recall_score(all_labels, all_preds)\n",
    "\n",
    "        writer.add_scalar(\"Loss/train\", running_loss / len(train_loader), epoch)\n",
    "        writer.add_scalar(\"Accuracy/val\", accuracy, epoch)\n",
    "        writer.add_scalar(\"F1/val\", f1, epoch)\n",
    "        writer.add_scalar(\"Precision/val\", precision, epoch)\n",
    "        writer.add_scalar(\"Recall/val\", recall, epoch)\n",
    "\n",
    "        with open(csv_path, mode='a', newline='') as f:\n",
    "            writer_csv = csv.writer(f)\n",
    "            writer_csv.writerow([epoch, accuracy, f1, precision, recall])\n",
    "\n",
    "        print(f\"Acc: {accuracy:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "\n",
    "        scheduler.step(accuracy)\n",
    "        trial.report(accuracy, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Save best model and hyperparameters\n",
    "    os.makedirs(\"../checkpoints\", exist_ok=True)\n",
    "    model_path = f\"../checkpoints/stage2_best_model_trial{trial.number}.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': best_model_state,\n",
    "        'hyperparameters': {\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'num_epochs': num_epochs,\n",
    "            'hidden_size': hidden_size,\n",
    "        }\n",
    "    }, model_path)\n",
    "    print(f\"Best model saved to: {model_path}\")\n",
    "\n",
    "    writer.close()\n",
    "    return best_accuracy\n"
   ],
   "id": "49921776ec3638c",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T11:35:25.420923Z",
     "start_time": "2025-04-18T05:48:23.663258Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-18 11:18:23,665] A new study created in memory with name: no-name-931d4e37-8f3e-41bc-aa14-f3cc76568c12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tuning...\n",
      "\n",
      " Trial 0 | LR: 2.5e-05 | BS: 64 | Epochs: 15 | Hidden: 320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 38/38 [10:30<00:00, 16.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7556 | F1: 0.7401 | Precision: 0.7904 | Recall: 0.6958\n",
      "\n",
      "📘 Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 38/38 [11:07<00:00, 17.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7506 | F1: 0.7763 | Precision: 0.7039 | Recall: 0.8653\n",
      "\n",
      "📘 Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 38/38 [10:52<00:00, 17.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7781 | F1: 0.7973 | Precision: 0.7338 | Recall: 0.8728\n",
      "\n",
      "📘 Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 38/38 [09:00<00:00, 14.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.8117 | F1: 0.8192 | Precision: 0.7880 | Recall: 0.8529\n",
      "\n",
      "📘 Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 38/38 [09:08<00:00, 14.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.8267 | F1: 0.8335 | Precision: 0.8018 | Recall: 0.8678\n",
      "\n",
      "📘 Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 38/38 [10:05<00:00, 15.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7930 | F1: 0.8052 | Precision: 0.7605 | Recall: 0.8554\n",
      "\n",
      "📘 Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 38/38 [10:48<00:00, 17.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.8142 | F1: 0.8198 | Precision: 0.7958 | Recall: 0.8454\n",
      "\n",
      "📘 Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 38/38 [10:06<00:00, 15.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7930 | F1: 0.7707 | Precision: 0.8638 | Recall: 0.6958\n",
      "⏹️ Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-18 12:42:09,931] Trial 0 finished with value: 0.8266832917705735 and parameters: {'learning_rate': 2.5441701240034406e-05, 'batch_size': 64, 'num_epochs': 15, 'hidden_size': 320}. Best is trial 0 with value: 0.8266832917705735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Best model saved to: ../checkpoints/stage2_best_model_trial0.pth\n",
      "\n",
      " Trial 1 | LR: 4.1e-05 | BS: 32 | Epochs: 6 | Hidden: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 76/76 [07:41<00:00,  6.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7618 | F1: 0.7516 | Precision: 0.7853 | Recall: 0.7207\n",
      "\n",
      "📘 Epoch 2/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 76/76 [09:06<00:00,  7.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7756 | F1: 0.7810 | Precision: 0.7625 | Recall: 0.8005\n",
      "\n",
      "📘 Epoch 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 76/76 [07:01<00:00,  5.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.8005 | F1: 0.8122 | Precision: 0.7672 | Recall: 0.8628\n",
      "\n",
      "📘 Epoch 4/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 76/76 [08:32<00:00,  6.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7606 | F1: 0.7318 | Precision: 0.8317 | Recall: 0.6534\n",
      "\n",
      "📘 Epoch 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 76/76 [08:32<00:00,  6.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.8005 | F1: 0.7878 | Precision: 0.8414 | Recall: 0.7406\n",
      "\n",
      "📘 Epoch 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 76/76 [07:19<00:00,  5.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7805 | F1: 0.7939 | Precision: 0.7483 | Recall: 0.8454\n",
      "⏹️ Early stopping at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-18 13:33:36,027] Trial 1 finished with value: 0.800498753117207 and parameters: {'learning_rate': 4.1171696049897376e-05, 'batch_size': 32, 'num_epochs': 6, 'hidden_size': 512}. Best is trial 0 with value: 0.8266832917705735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Best model saved to: ../checkpoints/stage2_best_model_trial1.pth\n",
      "\n",
      " Trial 2 | LR: 1.9e-05 | BS: 64 | Epochs: 10 | Hidden: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 38/38 [07:10<00:00, 11.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7070 | F1: 0.7296 | Precision: 0.6774 | Recall: 0.7905\n",
      "\n",
      "📘 Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 38/38 [09:59<00:00, 15.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7456 | F1: 0.7086 | Precision: 0.8294 | Recall: 0.6185\n",
      "\n",
      "📘 Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 38/38 [15:12<00:00, 24.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7556 | F1: 0.7586 | Precision: 0.7494 | Recall: 0.7681\n",
      "\n",
      "📘 Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 38/38 [13:03<00:00, 20.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7693 | F1: 0.7895 | Precision: 0.7259 | Recall: 0.8653\n",
      "\n",
      "📘 Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 38/38 [11:35<00:00, 18.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7930 | F1: 0.8042 | Precision: 0.7629 | Recall: 0.8504\n",
      "\n",
      "📘 Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 38/38 [13:05<00:00, 20.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7656 | F1: 0.7227 | Precision: 0.8845 | Recall: 0.6110\n",
      "\n",
      "📘 Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 38/38 [13:46<00:00, 21.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.8092 | F1: 0.8176 | Precision: 0.7831 | Recall: 0.8554\n",
      "\n",
      "📘 Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 38/38 [09:11<00:00, 14.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.8155 | F1: 0.8173 | Precision: 0.8093 | Recall: 0.8254\n",
      "\n",
      "📘 Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 38/38 [08:44<00:00, 13.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.8130 | F1: 0.8193 | Precision: 0.7925 | Recall: 0.8479\n",
      "\n",
      "📘 Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 38/38 [10:57<00:00, 17.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.8005 | F1: 0.8086 | Precision: 0.7770 | Recall: 0.8429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-18 15:29:16,655] Trial 2 finished with value: 0.8154613466334164 and parameters: {'learning_rate': 1.883397917600412e-05, 'batch_size': 64, 'num_epochs': 10, 'hidden_size': 512}. Best is trial 0 with value: 0.8266832917705735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Best model saved to: ../checkpoints/stage2_best_model_trial2.pth\n",
      "\n",
      " Trial 3 | LR: 8.5e-05 | BS: 32 | Epochs: 9 | Hidden: 448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 Epoch 1/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 76/76 [11:10<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7045 | F1: 0.6146 | Precision: 0.8832 | Recall: 0.4713\n",
      "\n",
      "📘 Epoch 2/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 76/76 [14:03<00:00, 11.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7506 | F1: 0.7333 | Precision: 0.7880 | Recall: 0.6858\n",
      "\n",
      "📘 Epoch 3/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 76/76 [09:02<00:00,  7.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7706 | F1: 0.7579 | Precision: 0.8022 | Recall: 0.7182\n",
      "\n",
      "📘 Epoch 4/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 76/76 [13:45<00:00, 10.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7519 | F1: 0.7872 | Precision: 0.6891 | Recall: 0.9177\n",
      "\n",
      "📘 Epoch 5/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 76/76 [09:03<00:00,  7.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7756 | F1: 0.7716 | Precision: 0.7855 | Recall: 0.7581\n",
      "\n",
      "📘 Epoch 6/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 76/76 [07:32<00:00,  5.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7768 | F1: 0.7591 | Precision: 0.8246 | Recall: 0.7032\n",
      "\n",
      "📘 Epoch 7/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 76/76 [08:22<00:00,  6.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7793 | F1: 0.7662 | Precision: 0.8146 | Recall: 0.7232\n",
      "\n",
      "📘 Epoch 8/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 76/76 [07:41<00:00,  6.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7506 | F1: 0.7863 | Precision: 0.6879 | Recall: 0.9177\n",
      "\n",
      "📘 Epoch 9/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 76/76 [08:00<00:00,  6.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc: 0.7968 | F1: 0.7985 | Precision: 0.7917 | Recall: 0.8055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-18 17:05:06,354] Trial 3 finished with value: 0.7967581047381546 and parameters: {'learning_rate': 8.494287124746428e-05, 'batch_size': 32, 'num_epochs': 9, 'hidden_size': 448}. Best is trial 0 with value: 0.8266832917705735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Best model saved to: ../checkpoints/stage2_best_model_trial3.pth\n",
      "\n",
      " Trial 4 | LR: 1.9e-04 | BS: 128 | Epochs: 5 | Hidden: 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/19 [00:11<?, ?it/s]\n",
      "[W 2025-04-18 17:05:19,205] Trial 4 failed with parameters: {'learning_rate': 0.00018916225997461672, 'batch_size': 128, 'num_epochs': 5, 'hidden_size': 192} because of the following error: RuntimeError('CUDA error: out of memory\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Prasanna Arjun\\AppData\\Local\\Temp\\ipykernel_11952\\597607420.py\", line 62, in objective\n",
      "    outputs = model(images)\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\", line 833, in forward\n",
      "    outputs = self.vit(\n",
      "              ^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\", line 616, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\", line 441, in forward\n",
      "    layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\", line 386, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\", line 328, in forward\n",
      "    self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\", line 248, in forward\n",
      "    query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "[W 2025-04-18 17:05:19,475] Trial 4 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mStarting tuning...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      2\u001B[39m study = optuna.create_study(direction=\u001B[33m'\u001B[39m\u001B[33mmaximize\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[43mstudy\u001B[49m\u001B[43m.\u001B[49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobjective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mBest trial: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudy.best_trial.number\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudy.best_value\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(study.best_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:475\u001B[39m, in \u001B[36mStudy.optimize\u001B[39m\u001B[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[39m\n\u001B[32m    373\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34moptimize\u001B[39m(\n\u001B[32m    374\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    375\u001B[39m     func: ObjectiveFuncType,\n\u001B[32m   (...)\u001B[39m\u001B[32m    382\u001B[39m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    383\u001B[39m ) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    384\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[32m    385\u001B[39m \n\u001B[32m    386\u001B[39m \u001B[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    473\u001B[39m \u001B[33;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[32m    474\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m475\u001B[39m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    476\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    477\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    478\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    479\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    480\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    481\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    482\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    483\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    484\u001B[39m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    485\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001B[39m, in \u001B[36m_optimize\u001B[39m\u001B[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[39m\n\u001B[32m     61\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     62\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs == \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     64\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     66\u001B[39m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     70\u001B[39m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     71\u001B[39m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     72\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     73\u001B[39m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     74\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     76\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs == -\u001B[32m1\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001B[39m, in \u001B[36m_optimize_sequential\u001B[39m\u001B[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[39m\n\u001B[32m    157\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m160\u001B[39m     frozen_trial = \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    162\u001B[39m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[32m    163\u001B[39m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[32m    164\u001B[39m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[32m    165\u001B[39m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[32m    166\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001B[39m, in \u001B[36m_run_trial\u001B[39m\u001B[34m(study, func, catch)\u001B[39m\n\u001B[32m    241\u001B[39m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mShould not reach.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    243\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    244\u001B[39m     frozen_trial.state == TrialState.FAIL\n\u001B[32m    245\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    246\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[32m    247\u001B[39m ):\n\u001B[32m--> \u001B[39m\u001B[32m248\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001B[39m, in \u001B[36m_run_trial\u001B[39m\u001B[34m(study, func, catch)\u001B[39m\n\u001B[32m    195\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001B[32m    196\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m197\u001B[39m         value_or_values = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    198\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions.TrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    199\u001B[39m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[32m    200\u001B[39m         state = TrialState.PRUNED\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 62\u001B[39m, in \u001B[36mobjective\u001B[39m\u001B[34m(trial)\u001B[39m\n\u001B[32m     59\u001B[39m optimizer.zero_grad()\n\u001B[32m     61\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m autocast(device_type=\u001B[33m'\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m'\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m62\u001B[39m     outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     63\u001B[39m     loss = criterion(outputs.logits, labels)\n\u001B[32m     65\u001B[39m scaler.scale(loss).backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:833\u001B[39m, in \u001B[36mViTForImageClassification.forward\u001B[39m\u001B[34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001B[39m\n\u001B[32m    825\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    826\u001B[39m \u001B[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[33;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[32m    828\u001B[39m \u001B[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[32m    829\u001B[39m \u001B[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[32m    830\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    831\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m--> \u001B[39m\u001B[32m833\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    834\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    835\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    836\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    837\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    838\u001B[39m \u001B[43m    \u001B[49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    839\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    840\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    842\u001B[39m sequence_output = outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    844\u001B[39m logits = \u001B[38;5;28mself\u001B[39m.classifier(sequence_output[:, \u001B[32m0\u001B[39m, :])\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:616\u001B[39m, in \u001B[36mViTModel.forward\u001B[39m\u001B[34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001B[39m\n\u001B[32m    610\u001B[39m     pixel_values = pixel_values.to(expected_dtype)\n\u001B[32m    612\u001B[39m embedding_output = \u001B[38;5;28mself\u001B[39m.embeddings(\n\u001B[32m    613\u001B[39m     pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding\n\u001B[32m    614\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m616\u001B[39m encoder_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    617\u001B[39m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    618\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    619\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    620\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    621\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    623\u001B[39m sequence_output = encoder_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    624\u001B[39m sequence_output = \u001B[38;5;28mself\u001B[39m.layernorm(sequence_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:441\u001B[39m, in \u001B[36mViTEncoder.forward\u001B[39m\u001B[34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m    434\u001B[39m     layer_outputs = \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(\n\u001B[32m    435\u001B[39m         layer_module.\u001B[34m__call__\u001B[39m,\n\u001B[32m    436\u001B[39m         hidden_states,\n\u001B[32m    437\u001B[39m         layer_head_mask,\n\u001B[32m    438\u001B[39m         output_attentions,\n\u001B[32m    439\u001B[39m     )\n\u001B[32m    440\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m441\u001B[39m     layer_outputs = \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    443\u001B[39m hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    445\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:386\u001B[39m, in \u001B[36mViTLayer.forward\u001B[39m\u001B[34m(self, hidden_states, head_mask, output_attentions)\u001B[39m\n\u001B[32m    380\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    381\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    382\u001B[39m     hidden_states: torch.Tensor,\n\u001B[32m    383\u001B[39m     head_mask: Optional[torch.Tensor] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    384\u001B[39m     output_attentions: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    385\u001B[39m ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n\u001B[32m--> \u001B[39m\u001B[32m386\u001B[39m     self_attention_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    387\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlayernorm_before\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# in ViT, layernorm is applied before self-attention\u001B[39;49;00m\n\u001B[32m    388\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    389\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    390\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    391\u001B[39m     attention_output = self_attention_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    392\u001B[39m     outputs = self_attention_outputs[\u001B[32m1\u001B[39m:]  \u001B[38;5;66;03m# add self attentions if we output attention weights\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:328\u001B[39m, in \u001B[36mViTAttention.forward\u001B[39m\u001B[34m(self, hidden_states, head_mask, output_attentions)\u001B[39m\n\u001B[32m    322\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    323\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    324\u001B[39m     hidden_states: torch.Tensor,\n\u001B[32m    325\u001B[39m     head_mask: Optional[torch.Tensor] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    326\u001B[39m     output_attentions: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    327\u001B[39m ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n\u001B[32m--> \u001B[39m\u001B[32m328\u001B[39m     self_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    330\u001B[39m     attention_output = \u001B[38;5;28mself\u001B[39m.output(self_outputs[\u001B[32m0\u001B[39m], hidden_states)\n\u001B[32m    332\u001B[39m     outputs = (attention_output,) + self_outputs[\u001B[32m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:248\u001B[39m, in \u001B[36mViTSelfAttention.forward\u001B[39m\u001B[34m(self, hidden_states, head_mask, output_attentions)\u001B[39m\n\u001B[32m    246\u001B[39m key_layer = \u001B[38;5;28mself\u001B[39m.transpose_for_scores(\u001B[38;5;28mself\u001B[39m.key(hidden_states))\n\u001B[32m    247\u001B[39m value_layer = \u001B[38;5;28mself\u001B[39m.transpose_for_scores(\u001B[38;5;28mself\u001B[39m.value(hidden_states))\n\u001B[32m--> \u001B[39m\u001B[32m248\u001B[39m query_layer = \u001B[38;5;28mself\u001B[39m.transpose_for_scores(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    250\u001B[39m attention_interface: Callable = eager_attention_forward\n\u001B[32m    251\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config._attn_implementation != \u001B[33m\"\u001B[39m\u001B[33meager\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Lungdisease\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 18,
   "source": [
    "print(\"Starting tuning...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "print(f\"Best trial: {study.best_trial.number} | Accuracy: {study.best_value:.4f}\")\n",
    "print(study.best_params)"
   ],
   "id": "bacf1244d07b9f89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T11:36:55.801221Z",
     "start_time": "2025-04-18T11:36:55.795079Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: FrozenTrial(number=0, state=1, values=[0.8266832917705735], datetime_start=datetime.datetime(2025, 4, 18, 11, 18, 23, 666259), datetime_complete=datetime.datetime(2025, 4, 18, 12, 42, 9, 923254), params={'learning_rate': 2.5441701240034406e-05, 'batch_size': 64, 'num_epochs': 15, 'hidden_size': 320}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.7556109725685786, 1: 0.7506234413965087, 2: 0.7780548628428927, 3: 0.8117206982543641, 4: 0.8266832917705735, 5: 0.7930174563591023, 6: 0.814214463840399, 7: 0.7930174563591023}, distributions={'learning_rate': FloatDistribution(high=0.001, log=True, low=1e-05, step=None), 'batch_size': CategoricalDistribution(choices=(32, 64, 128)), 'num_epochs': IntDistribution(high=15, log=False, low=5, step=1), 'hidden_size': IntDistribution(high=512, log=False, low=128, step=64)}, trial_id=0, value=None) | Accuracy: 0.8267\n"
     ]
    }
   ],
   "execution_count": 20,
   "source": "print(f\"Best trial: {study.best_trial} | Accuracy: {study.best_value:.4f}\")",
   "id": "efcbddd1b897390c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T17:56:53.891186Z",
     "start_time": "2025-04-18T17:56:53.863262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = torch.load('/checkpoints/stage2_best_model_trial0.pth', map_location='cpu')\n",
    "print(checkpoint['hyperparameters'])  # If saved\n"
   ],
   "id": "d56797861840d087",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m best_trial = \u001B[43mstudy\u001B[49m.trials[\u001B[32m0\u001B[39m]  \u001B[38;5;66;03m# or use study.best_trial\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(best_trial.params)\n",
      "\u001B[31mNameError\u001B[39m: name 'study' is not defined"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
